---
title: CUDA
date: 2024-08-29 11:33:00 +0800
categories: [Tech]
tags: [tech]
pin: true
math: true
mermaid: true
---

I have stumbled upon CUDA in the past when running ML stuff on colab "cuda.is_available()" but I until now, I haven't stopped to question what CUDA is. But while reading about Python's Global Interpreter Lock, I stumbled upon it. This statement got me a bit more interested to know what CUDA is about: 
"[...] creating a DataLoader after accessing a GPU can lead to confusing CUDA errors. Accessing GPUs within a DataLoader worker quickly leads to out-of-memory errors because processes do not share CUDA contexts (unlike threads within a process)."
source: https://peps.python.org/pep-0703/

## What does CUDA stand for?
Compute Unified Device Architecture

## What is CUDA?
Developed by NVIDIA
NVIDIA's CUDA programming model used for parallel computing on NVIDIA's GPUs (needs NVIDIA GPU to be used)
Helpful in DNN
Includes Runtime, Drivers, Compilers, and Developer Tools
Allows employing of GPU for tasks GPUs are better at (e.g. matrix multiplication) to speed up computation

## What is a CUDA context?
The environment in which CUDA operations take place. Think of it as a container for resources, state information, and memory management related to GPU operations.

CUDA contexts are generally associated with a particular thread to ensure thread safety.

c++ code example:

```
// Initialize CUDA context
cudaSetDevice(0); // Select GPU device
cudaDeviceReset(); // Initialize and create a new CUDA context

// Allocate device memory
cudaMalloc(&deviceArray, size);

// Copy data from host to device
cudaMemcpy(deviceArray, hostArray, size, cudaMemcpyHostToDevice);

// Launch kernel on the GPU
myKernel<<<blocks, threads>>>(deviceArray);

// Copy results from device to host
cudaMemcpy(hostResult, deviceResult, size, cudaMemcpyDeviceToHost);

// Free device memory
cudaFree(deviceArray);
```
